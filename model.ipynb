{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97a0d1fe",
   "metadata": {},
   "source": [
    "# SafeStep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c5816d",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68349639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# 경로 설정\n",
    "TRAIN_CSV = 'data/train/train.csv'\n",
    "TEST_CSV = 'data/test/test.csv'\n",
    "MODEL_PATH = 'models/sensor_model.h5'\n",
    "SCALER_PATH = 'models/scaler.pkl'\n",
    "os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b32839",
   "metadata": {},
   "source": [
    "## 2. 데이터 로드 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d7b848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess(train_csv: str, test_csv: str, scaler_path: str = SCALER_PATH):\n",
    "    \"\"\"\n",
    "    CSV 파일로부터 데이터를 로드하고 전처리(표준화)하여 반환합니다.\n",
    "\n",
    "    Returns:\n",
    "        X_train, y_train, X_test, y_test, scaler\n",
    "    \"\"\"\n",
    "    # 데이터 로드\n",
    "    df_train = pd.read_csv(train_csv)\n",
    "    df_test = pd.read_csv(test_csv)\n",
    "\n",
    "    # 피처와 레이블 분리\n",
    "    X_train = df_train.iloc[:, :-1].values\n",
    "    y_train = df_train.iloc[:, -1].values\n",
    "    X_test = df_test.iloc[:, :-1].values\n",
    "    y_test = df_test.iloc[:, -1].values\n",
    "\n",
    "    # 표준화 수행\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # 스케일러 저장\n",
    "    import pickle\n",
    "    with open(scaler_path, 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, scaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89676496",
   "metadata": {},
   "source": [
    "## 3. 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bef865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    model_path: str = MODEL_PATH,\n",
    "    epochs: int = 20,\n",
    "    batch_size: int = 32\n",
    ") -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    전처리된 데이터를 이용해 모델을 학습하고 저장합니다.\n",
    "    Returns: 학습된 모델\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(len(np.unique(y_train)), activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size)\n",
    "    model.save(model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b339c3c2",
   "metadata": {},
   "source": [
    "## 4. 모델 로드 및 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c8f807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_path(model_path: str = MODEL_PATH) -> tf.keras.Model:\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
    "    return tf.keras.models.load_model(model_path)\n",
    "\n",
    "def predict_step(model: tf.keras.Model, data_list: list[float], scaler: StandardScaler) -> int:\n",
    "    arr = np.array(data_list, dtype=float).reshape(1, -1)\n",
    "    arr = scaler.transform(arr)\n",
    "    preds = model.predict(arr)\n",
    "    return int(np.argmax(preds, axis=1)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5cc96d",
   "metadata": {},
   "source": [
    "## 5. 학습·예측·평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f3996a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # 데이터 로드 및 전처리\n",
    "    X_train, y_train, X_test, y_test, scaler = load_and_preprocess(TRAIN_CSV, TEST_CSV)\n",
    "    # 모델 학습\n",
    "    model = train_model(X_train, y_train, epochs=10, batch_size=16)\n",
    "    # 테스트 데이터 예측\n",
    "    preds = []\n",
    "    for xi in X_test:\n",
    "        preds.append(predict_step(model, xi.tolist(), scaler))\n",
    "\n",
    "    # 평가 지표 출력\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    print(f'Accuracy: {acc:.4f}')\n",
    "    print('Confusion Matrix:\\n', cm)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
